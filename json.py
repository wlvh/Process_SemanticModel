# -*- coding: utf-8 -*-
"""
LLMModelDocLite â€” è¯­ä¹‰æ¨¡å‹â€œè½»æ–‡æ¡£ + æ•°æ®æ¢ç´¢â€ç”Ÿæˆå™¨ï¼ˆå•æ–‡ä»¶å®Œæ•´ç‰ˆï¼‰
- è¾“å‡ºä¸ºç´§å‡‘ JSON å¥‘çº¦ï¼Œä¸“ä¾› LLM æ¶ˆè´¹ï¼ˆä¸å†™é•¿ç¯‡èƒŒæ™¯ï¼‰
- å…ƒæ•°æ®å–è‡ª INFO.VIEW.*ï¼ˆå¯é€‰ï¼šæœ€å°é™çº§ï¼‰
- è¡¨åˆ†ç±»ä¸æ˜Ÿå‹å›¾ï¼ˆä»…ä¸šåŠ¡ç›¸å…³ï¼‰
- è½»é‡æ•°æ®æ¢ç´¢ï¼ˆå¯é€‰ï¼‰ï¼š
  * äº‹å®è¡¨ row_count
  * é”šç‚¹ï¼ˆAnchorï¼‰= via-key åˆ° DimDate çš„ MAX(date) + 7/30/90 æ—¥çª—å£è®¡æ•°
  * å¤±è´¥å›é€€ directï¼ˆäº‹å®è¡¨æ—¥æœŸåˆ—ï¼‰/fallbackï¼ˆå…¨å±€ DimDateï¼‰
- ç²¾ç®€å…³ç³»ä½“æ£€ï¼ˆå¯é€‰ï¼ŒTop-Kï¼‰ï¼šFK ç©ºå€¼ç‡ã€è¦†ç›–ç‡ï¼ˆå­¤å„¿ç‡ï¼‰ï¼ŒRED/YELLOW/GREEN
- ç»´åº¦å±•ç¤ºåˆ—ä¸åˆ«åæ˜ å°„ï¼ˆä¸­è‹±/å¸¸ç”¨æœ¯è¯­ï¼‰
- åº¦é‡åˆ†ç±»ä¸ä¾èµ–ï¼ˆä¸å±•å¼€ DAXï¼Œé¿å…å†—é•¿ï¼›å¯é€šè¿‡å¼€å…³åŒ…å«ï¼‰

ä¾èµ–ï¼š
  pip install pandas
  ï¼ˆåœ¨ Fabric/Power BI Notebook ç¯å¢ƒï¼‰pip install sempy

è¾“å‡ºï¼š
  ./llm_contract.json
"""

from __future__ import annotations
import datetime as dt
import re
import json
import time
from typing import Any, Dict, List, Optional, Protocol, Tuple, Set

import pandas as pd

try:
    import numpy as np
    _NUMPY_AVAILABLE = True
except ImportError:
    np = None
    _NUMPY_AVAILABLE = False

# ----------------------------
# å¯é€‰ï¼šFabric SDK
# ----------------------------
try:
    import sempy.fabric as fabric  # Microsoft Fabric / Power BI Python SDK
    _FABRIC_AVAILABLE = True
except Exception:
    fabric = None
    _FABRIC_AVAILABLE = False


# ----------------------------
# Runnerï¼ˆå¯ä¾èµ–æ³¨å…¥ï¼‰
# ----------------------------
class DaxQueryRunner(Protocol):
    def evaluate(self, dataset: str, dax: str, workspace: Optional[str]) -> pd.DataFrame: ...


class FabricRunner:
    """é»˜è®¤ Runnerï¼šsempy.fabric.evaluate_daxï¼Œå¸¦è½»é‡é‡è¯•ã€‚"""
    def __init__(self, retries: int = 2, backoff: float = 0.8):
        self.retries = max(0, retries)
        self.backoff = max(0.0, backoff)

    def evaluate(self, dataset: str, dax: str, workspace: Optional[str]) -> pd.DataFrame:
        if not _FABRIC_AVAILABLE:
            raise RuntimeError("Fabric SDK ä¸å¯ç”¨ã€‚è¯·åœ¨æ”¯æŒçš„ç¯å¢ƒå®‰è£… `sempy` å¹¶è¿è¡Œã€‚")
        last_err = None
        for i in range(self.retries + 1):
            try:
                return fabric.evaluate_dax(dataset=dataset, dax_string=dax, workspace=workspace)
            except Exception as e:
                last_err = e
                if i < self.retries:
                    time.sleep(self.backoff * (i + 1))
        # æœ€åä¸€æ¬¡å¤±è´¥æŠ›å‡º
        raise last_err


# ----------------------------
# ä¸»ç±»ï¼šLite æ–‡æ¡£ + æ•°æ®æ¢ç´¢
# ----------------------------
class LLMModelDocLite:
    def __init__(self, runner: Optional[DaxQueryRunner] = None, verbose: bool = True):
        self.runner = runner or FabricRunner()
        self.verbose = verbose

    # ======== å¯¹å¤–å…¥å£ ========
    def generate(
        self,
        model_name: str,
        workspace: Optional[str] = None,
        *,
        include_measure_dax: bool = False,   # True åˆ™éšå¥‘çº¦è¾“å‡º DAXï¼ˆå¯èƒ½è¾ƒé•¿ï¼‰
        profile_mode: str = "light",         # "off" | "light" | "standard"
        relationship_top_k: int = 12,
        include_enums: bool = False,
        max_enum_values: int = 10
    ) -> str:
        md = self._fetch_metadata(model_name, workspace)
        st = self._analyze(md)

        # è½»é‡æ•°æ®æ¢ç´¢
        data_profile: Dict[str, Any] = {}
        if profile_mode in {"light", "standard"}:
            if self.verbose: print("ğŸ©º æ•°æ®æ¢ç´¢ï¼ˆfacts via-key + row_countï¼‰...")
            data_profile["facts"] = self._profile_facts_via_key(model_name, workspace, md, st)
            if profile_mode == "standard":
                if self.verbose: print("ğŸ§ª å…³ç³»ä½“æ£€ï¼ˆTop-K è¾¹ï¼‰...")
                data_profile["relationships"] = self._profile_relationships_lite(
                    model_name, workspace, md, st, top_k=relationship_top_k
                )

        # ç»„è£… LLM å¥‘çº¦
        contract = self._build_llm_contract(
            md=md,
            st=st,
            profiles=data_profile,
            include_measure_dax=include_measure_dax,
            include_enums=include_enums,
            max_enum_values=max_enum_values
        )
        return self._json_dumps(contract)

    # ======== å…ƒæ•°æ®æå– ========
    def _fetch_metadata(self, model_name: str, workspace: Optional[str]) -> Dict[str, Any]:
        if self.verbose: print("ğŸ“Š æå–å…ƒæ•°æ®ï¼ˆINFO.VIEW.*ï¼‰...")
        md: Dict[str, Any] = {'tables': [], 'columns': [], 'measures': [], 'relationships': [], 'errors': []}

        queries = {
            'tables': """EVALUATE SELECTCOLUMNS(
                INFO.VIEW.TABLES(),
                "table_name",[Name],
                "is_hidden",[IsHidden],
                "description",[Description],
                "storage_mode",[StorageMode]
            )""",
            'columns': """EVALUATE SELECTCOLUMNS(
                INFO.VIEW.COLUMNS(),
                "table_name",[Table],
                "column_name",[Name],
                "data_type",[DataType],
                "is_hidden",[IsHidden],
                "is_key",[IsKey],
                "is_nullable",[IsNullable],
                "is_unique",[IsUnique],
                "description",[Description]
            )""",
            'measures': """EVALUATE SELECTCOLUMNS(
                INFO.VIEW.MEASURES(),
                "table_name",[Table],
                "measure_name",[Name],
                "dax_expression",[Expression],
                "format_string",[FormatString],
                "is_hidden",[IsHidden],
                "description",[Description]
            )""",
            'relationships': """EVALUATE SELECTCOLUMNS(
                INFO.VIEW.RELATIONSHIPS(),
                "from_table",[FromTable],
                "from_column",[FromColumn],
                "from_cardinality",[FromCardinality],
                "to_table",[ToTable],
                "to_column",[ToColumn],
                "to_cardinality",[ToCardinality],
                "is_active",[IsActive],
                "cross_filter_direction",[CrossFilteringBehavior]
            )"""
        }

        for key, dax in queries.items():
            try:
                df = self.runner.evaluate(model_name, dax, workspace)
                df = self._normalize_df(df)
                md[key] = df.to_dict('records')
                if self.verbose: print(f"  âœ“ {key}: {len(md[key])}")
            except Exception as e:
                md['errors'].append(f"{key} not available: {e}")
                if self.verbose: print(f"  âš  {key} å¤±è´¥ï¼ˆç•¥è¿‡ï¼‰: {e}")

        # æ ‡æ³¨ä¸šåŠ¡è¡¨ï¼ˆå‰”é™¤è‡ªåŠ¨æ—¥æœŸè¡¨ + éšè—è¡¨ï¼‰
        auto_date_regex = re.compile(r'^(LocalDateTable_|DateTableTemplate_)', re.IGNORECASE)
        md['auto_date_tables'] = [
            t.get('table_name') for t in md['tables']
            if auto_date_regex.match(t.get('table_name') or '')
        ]
        md['business_tables'] = [
            t for t in md['tables']
            if not auto_date_regex.match(t.get('table_name') or '') and not self._b(t.get('is_hidden'))
        ]
        return md

    @staticmethod
    def _normalize_df(df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()
        df.columns = [
            (c or '').strip().replace('[', '').replace(']', '').lower().replace(' ', '_')
            for c in df.columns
        ]
        return df

    # ======== ç»“æ„åˆ†æ ========
    def _analyze(self, md: Dict[str, Any]) -> Dict[str, Any]:
        st: Dict[str, Any] = {'table_types': {}, 'star': {}, 'fact_time': {}}

        # é€‰æ‹©å…¨å±€ DimDate
        dim_table, dim_key, dim_date_col = self._pick_default_dimdate(md)
        st['date_axis'] = {'table': dim_table, 'key_column': dim_key, 'date_column': dim_date_col}

        # è¡¨åˆ†ç±»ï¼ˆç®€å•è§„åˆ™ + å…³ç³»ä¿¡å·ï¼‰
        col_by_table = {}
        for c in md['columns']:
            col_by_table.setdefault(c.get('table_name'), []).append(c)
        rels = [r for r in md['relationships'] if self._active_business_rel(r)]

        # ç»Ÿè®¡å…³ç³»æ–¹å‘
        out_count, in_count = {}, {}
        for r in rels:
            out_count[r['from_table']] = out_count.get(r['from_table'], 0) + 1
            in_count[r['to_table']] = in_count.get(r['to_table'], 0) + 1

        for t in [tb.get('table_name') for tb in md['business_tables']]:
            name = (t or '').lower()
            outgoing = out_count.get(t, 0)
            incoming = in_count.get(t, 0)
            cols = col_by_table.get(t, [])
            # å‘½åä¼˜å…ˆ
            if name.startswith('fact') or name.startswith('vwpcse_fact'):
                st['table_types'][t] = 'fact'
            elif name.startswith('dim') or name.startswith('vwpcse_dim'):
                st['table_types'][t] = 'dimension'
            else:
                # ç»“æ„ä¿¡å·
                if outgoing >= 2:
                    st['table_types'][t] = 'fact'
                elif incoming > outgoing:
                    st['table_types'][t] = 'dimension'
                else:
                    # æ–‡æœ¬åˆ—å¤š -> ç»´åº¦
                    num_text = sum('text' in (c.get('data_type') or '').lower() or 'string' in (c.get('data_type') or '').lower() for c in cols)
                    num_num = sum(any(k in (c.get('data_type') or '').lower() for k in ['int','decimal','double','currency','number','whole']) for c in cols)
                    st['table_types'][t] = 'dimension' if num_text > num_num else 'other'

        # æ˜Ÿå‹å›¾ï¼ˆäº‹å® -> ç»´åº¦ï¼‰
        fact_tables = [t for t, ty in st['table_types'].items() if ty == 'fact']
        dim_tables = [t for t, ty in st['table_types'].items() if ty == 'dimension']
        for f in fact_tables:
            st['star'][f] = {'dimensions': []}
        for r in rels:
            fr, to = r.get('from_table'), r.get('to_table')
            if fr in fact_tables and to in dim_tables:
                st['star'][fr]['dimensions'].append({
                    'dimension_table': to,
                    'join_key': f"{r.get('from_column')} â†’ {r.get('to_column')}",
                    'direction': r.get('cross_filter_direction')
                })

        # æ¯ä¸ªäº‹å®çš„â€œé»˜è®¤æ—¶é—´é”®â€ä¸â€œæ—¥æœŸè½´â€
        for f in fact_tables:
            # æŸ¥æ‰¾ä¸å…¨å±€/å€™é€‰ DimDate çš„å…³ç³»
            pick = self._find_time_key_for_fact(f, rels, dim_tables, prefer_table=dim_table)
            if pick:
                st['fact_time'][f] = {
                    'default_time_key': pick[0],      # fact key
                    'date_dimension': pick[1],        # dim table
                    'date_dimension_key': pick[2],    # dim key
                }
            else:
                # æ²¡æœ‰é”®ï¼Œåˆ™å°è¯•äº‹å®è¡¨å†…æ—¥æœŸåˆ—ï¼ˆä»…è®°å½•ï¼ŒçœŸæ­£ profiling æ—¶å†å¤„ç†ï¼‰
                st['fact_time'][f] = {'default_time_key': None, 'date_dimension': dim_table, 'date_dimension_key': dim_key}
        return st

    def _pick_default_dimdate(self, md: Dict[str, Any]) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """é€‰æ‹©å…¨å±€ DimDate è¡¨ã€é”®ã€æ—¥æœŸåˆ—ã€‚"""
        candidates = [t.get('table_name') for t in md['tables'] if t.get('table_name')]
        # ä¼˜å…ˆï¼šåŒ…å« dimdate/date/calendar çš„ç»´åº¦
        def score(name: str) -> int:
            n = name.lower()
            if 'dimdate' in n: return 0
            if n.endswith('dimdate'): return 1
            if 'calendar' in n: return 2
            if n.endswith('date'): return 3
            if 'date' in n: return 4
            return 9
        candidates = sorted(candidates, key=score)
        dim_table = None
        for t in candidates:
            if 'dim' in t.lower() and ('date' in t.lower() or 'calendar' in t.lower()):
                dim_table = t; break
        if not dim_table:
            # æ¬¡é€‰ï¼šåŒ…å« date/calendar çš„ä»»ä½•è¡¨
            for t in candidates:
                if 'date' in t.lower() or 'calendar' in t.lower():
                    dim_table = t; break

        # é€‰æ‹©é”®ä¸æ—¥æœŸåˆ—
        dim_key = None
        dim_date_col = None
        if dim_table:
            cols = [c for c in md['columns'] if c.get('table_name') == dim_table]
            names = [c.get('column_name') for c in cols if c.get('column_name')]
            # é”®
            for cand in ['DateKey', 'Date Id', 'DateID']:
                if cand in names: dim_key = cand; break
            if not dim_key:
                # endswith DateKey
                m = [n for n in names if n and n.lower().endswith('datekey')]
                dim_key = m[0] if m else (names[0] if names else None)
            # æ—¥æœŸåˆ—
            dim_date_col = self._select_dim_date_col(dim_table, md)
        return dim_table, dim_key, dim_date_col

    def _select_dim_date_col(self, dim_table: str, md: Dict[str, Any]) -> Optional[str]:
        cols = [c for c in md['columns'] if c.get('table_name') == dim_table]
        names = [c.get('column_name') for c in cols if c.get('column_name')]
        typed = [c.get('column_name') for c in cols if 'date' in (c.get('data_type') or '').lower()]
        for prefer in ['CalendarDate', 'Date', 'DateValue']:
            if prefer in names: return prefer
        return typed[0] if typed else (names[0] if names else None)

    def _find_time_key_for_fact(
        self, fact: str, rels: List[Dict[str, Any]], dim_tables: List[str], prefer_table: Optional[str]
    ) -> Optional[Tuple[str, str, str]]:
        """åœ¨å…³ç³»ä¸­æ‰¾ äº‹å®->æ—¥æœŸç»´åº¦ çš„é”®ã€‚"""
        pick: Optional[Tuple[str, str, str]] = None
        # é¦–é€‰ï¼šæŒ‡å‘ prefer_table çš„å…³ç³»
        for r in rels:
            if r.get('from_table') == fact and r.get('to_table') == prefer_table:
                from_col = r.get('from_column'); to_col = r.get('to_column') or 'DateKey'
                if from_col: return (from_col, r.get('to_table'), to_col)
        # æ¬¡é€‰ï¼šä»»ä½•æ—¥æœŸå‹ç»´åº¦
        for r in rels:
            if r.get('from_table') == fact and r.get('to_table') in dim_tables and any(
                k in (r.get('to_table') or '').lower() for k in ['dimdate', 'calendar', 'date']
            ):
                from_col = r.get('from_column'); to_col = r.get('to_column') or 'DateKey'
                if from_col: pick = (from_col, r.get('to_table'), to_col); break
        return pick

    # ======== æ•°æ®æ¢ç´¢ï¼šfactsï¼ˆvia-keyï¼‰ ========
    def _profile_facts_via_key(
        self, model_name: str, workspace: Optional[str], md: Dict[str, Any], st: Dict[str, Any]
    ) -> Dict[str, Any]:
        out: Dict[str, Any] = {}
        dim_table = st['date_axis']['table']
        dim_key = st['date_axis']['key_column']
        dim_date = st['date_axis']['date_column']

        # äº‹å®è¡¨åˆ—è¡¨
        fact_tables = [t for t, ty in st['table_types'].items() if ty == 'fact']

        for fact in fact_tables:
            # 1) è¡Œæ•°
            rc = None
            try:
                df = self.runner.evaluate(model_name, f"EVALUATE ROW(\"row_count\", COUNTROWS('{fact}'))", workspace)
                rc = int(df.iloc[0, 0]) if not df.empty else None
            except Exception:
                pass

            # 2) time anchorï¼ˆä¼˜å…ˆ via-keyï¼‰
            cfg = st['fact_time'].get(fact, {}) if st.get('fact_time') else {}
            fkey = cfg.get('default_time_key')
            dtab = cfg.get('date_dimension') or dim_table
            dkey = cfg.get('date_dimension_key') or dim_key
            dcol = self._select_dim_date_col(dtab, md) if dtab else dim_date

            payload = {"source": None, "min": None, "max": None, "anchor": None,
                       "cnt7": None, "cnt30": None, "cnt90": None, "date_axis_column": dcol}

            # a) via-key
            if fkey and dtab and dkey and dcol:
                dax = f"""
EVALUATE
VAR K =
  SELECTCOLUMNS(
    FILTER(VALUES('{fact}'[{fkey}]), NOT ISBLANK('{fact}'[{fkey}])),
    "__k", '{fact}'[{fkey}]
  )
VAR Anchor =
  CALCULATE(MAX('{dtab}'[{dcol}]), TREATAS(K, '{dtab}'[{dkey}]))
VAR MinDate =
  CALCULATE(MIN('{dtab}'[{dcol}]), TREATAS(K, '{dtab}'[{dkey}]))
VAR W7  = IF(NOT ISBLANK(Anchor), FILTER(ALL('{dtab}'[{dcol}]), '{dtab}'[{dcol}]>Anchor-7  && '{dtab}'[{dcol}]<=Anchor))
VAR W30 = IF(NOT ISBLANK(Anchor), FILTER(ALL('{dtab}'[{dcol}]), '{dtab}'[{dcol}]>Anchor-30 && '{dtab}'[{dcol}]<=Anchor))
VAR W90 = IF(NOT ISBLANK(Anchor), FILTER(ALL('{dtab}'[{dcol}]), '{dtab}'[{dcol}]>Anchor-90 && '{dtab}'[{dcol}]<=Anchor))
VAR W7K   = CALCULATETABLE(VALUES('{dtab}'[{dkey}]),  W7)
VAR W30K  = CALCULATETABLE(VALUES('{dtab}'[{dkey}]), W30)
VAR W90K  = CALCULATETABLE(VALUES('{dtab}'[{dkey}]), W90)
RETURN
ROW(
  "min",     MinDate,
  "max",     Anchor,
  "anchor",  Anchor,
  "cnt7",    CALCULATE(COUNTROWS('{fact}'), TREATAS(W7K,  '{fact}'[{fkey}])),
  "cnt30",   CALCULATE(COUNTROWS('{fact}'), TREATAS(W30K, '{fact}'[{fkey}])),
  "cnt90",   CALCULATE(COUNTROWS('{fact}'), TREATAS(W90K, '{fact}'[{fkey}]))
)
"""
                try:
                    df = self.runner.evaluate(model_name, dax, workspace)
                    if not df.empty and pd.notna(df.iloc[0].get("anchor")):
                        r = df.iloc[0].to_dict()
                        payload.update({
                            "min": self._to_iso(r.get("min")),
                            "max": self._to_iso(r.get("max")),
                            "anchor": self._to_iso(r.get("anchor")),
                            "cnt7": self._to_int(r.get("cnt7")),
                            "cnt30": self._to_int(r.get("cnt30")),
                            "cnt90": self._to_int(r.get("cnt90"))
                        })
                        payload["source"] = "via_key"
                except Exception:
                    pass

            # b) directï¼ˆäº‹å®è¡¨å†…é¦–ä¸ªæ—¥æœŸå‹åˆ—ï¼‰
            if payload["source"] is None:
                date_cols = [c.get("column_name") for c in md["columns"]
                             if c.get("table_name") == fact and 'date' in (c.get("data_type") or "").lower()]
                if date_cols:
                    col = date_cols[0]
                    dax = f"""
EVALUATE
VAR B = FILTER(ALL('{fact}'[{col}]), NOT ISBLANK('{fact}'[{col}]))
VAR Mi = MINX(B, '{fact}'[{col}])
VAR Ma = MAXX(B, '{fact}'[{col}])
VAR W7  = IF(NOT ISBLANK(Ma), FILTER(B, '{fact}'[{col}]>Ma-7  && '{fact}'[{col}]<=Ma))
VAR W30 = IF(NOT ISBLANK(Ma), FILTER(B, '{fact}'[{col}]>Ma-30 && '{fact}'[{col}]<=Ma))
VAR W90 = IF(NOT ISBLANK(Ma), FILTER(B, '{fact}'[{col}]>Ma-90 && '{fact}'[{col}]<=Ma))
RETURN
ROW(
  "min", Mi, "max", Ma, "anchor", Ma,
  "cnt7", COUNTROWS(W7), "cnt30", COUNTROWS(W30), "cnt90", COUNTROWS(W90)
)
"""
                    try:
                        df = self.runner.evaluate(model_name, dax, workspace)
                        if not df.empty and pd.notna(df.iloc[0].get("anchor")):
                            r = df.iloc[0].to_dict()
                            payload.update({
                                "min": self._to_iso(r.get("min")),
                                "max": self._to_iso(r.get("max")),
                                "anchor": self._to_iso(r.get("anchor")),
                                "cnt7": self._to_int(r.get("cnt7")),
                                "cnt30": self._to_int(r.get("cnt30")),
                                "cnt90": self._to_int(r.get("cnt90"))
                            })
                            payload["source"] = "direct"
                    except Exception:
                        pass

            # c) fallbackï¼ˆå…¨å±€ DimDateï¼‰
            if payload["source"] is None and dim_table and dim_date:
                try:
                    df = self.runner.evaluate(model_name, f"EVALUATE ROW(\"anchor\", MAX('{dim_table}'[{dim_date}]))", workspace)
                    if not df.empty:
                        v = df.iloc[0, 0]
                        iso = self._to_iso(v)
                        payload["anchor"] = iso
                        payload["max"] = iso
                        payload["source"] = "fallback"
                except Exception:
                    pass

            out[fact] = {"row_count": rc, "time": payload}
        return out

    # ======== æ•°æ®æ¢ç´¢ï¼šå…³ç³»ä½“æ£€ï¼ˆTop-Kï¼‰ ========
    def _profile_relationships_lite(
        self, model_name: str, workspace: Optional[str], md: Dict[str, Any], st: Dict[str, Any], top_k: int = 12
    ) -> Dict[str, Any]:
        # å€™é€‰è¾¹ï¼šäº‹å®->ï¼ˆå¸¸è§ç»´åº¦ï¼‰æ´»åŠ¨å…³ç³»
        common_dims = ("dimdate","dimgeography","dimqueue","dimpartner","dimsap","dimlanguage","dimsite","dimrootcause")
        active_edges = [r for r in md['relationships'] if self._active_business_rel(r)]
        candidates = []
        for r in active_edges:
            to = (r.get('to_table') or '').lower()
            if any(k in to for k in common_dims):
                candidates.append(r)
        if not candidates:
            candidates = active_edges[:]
        edges = candidates[:max(1, top_k)]

        details: List[Dict[str, Any]] = []
        for r in edges:
            ftab, fcol, dtab, dcol = r.get("from_table"), r.get("from_column"), r.get("to_table"), r.get("to_column")

            # ç©ºå€¼ç‡/è¡Œæ•°/å»é‡
            blank_ratio = coverage = None
            blank_fk = total_rows = distinct_fk = None
            dax1 = f"""
EVALUATE ROW(
 "blank_fk", COUNTROWS(FILTER('{ftab}', ISBLANK('{ftab}'[{fcol}]))),
 "total_rows", COUNTROWS('{ftab}'),
 "distinct_fk", DISTINCTCOUNT('{ftab}'[{fcol}])
)"""
            try:
                df1 = self.runner.evaluate(model_name, dax1, workspace)
                if not df1.empty:
                    row = df1.iloc[0]
                    blank_fk   = self._to_int(row.get("blank_fk"))
                    total_rows = self._to_int(row.get("total_rows"))
                    distinct_fk= self._to_int(row.get("distinct_fk"))
                    if total_rows and blank_fk is not None:
                        blank_ratio = blank_fk / total_rows
            except Exception:
                pass

            # å­¤å„¿é”®ï¼ˆFact ä¸­æœ‰ã€Dim ä¸­æ²¡æœ‰ï¼‰
            orphan_fk = None
            dax2 = f"""
EVALUATE
VAR FK = SELECTCOLUMNS(FILTER(VALUES('{ftab}'[{fcol}]), NOT ISBLANK('{ftab}'[{fcol}])), "__k", '{ftab}'[{fcol}])
VAR PK = SELECTCOLUMNS(FILTER(VALUES('{dtab}'[{dcol}]), NOT ISBLANK('{dtab}'[{dcol}])), "__k", '{dtab}'[{dcol}])
RETURN ROW("orphan_fk", COUNTROWS(EXCEPT(FK, PK)))
"""
            try:
                df2 = self.runner.evaluate(model_name, dax2, workspace)
                if not df2.empty:
                    orphan_fk = self._to_int(df2.iloc[0].get("orphan_fk"))
                    if distinct_fk and distinct_fk > 0 and orphan_fk is not None:
                        coverage = 1 - (orphan_fk / distinct_fk)
            except Exception:
                pass

            # ä¸¥é‡åº¦
            sev = "GREEN"
            if (coverage is not None and coverage < 0.95) or (blank_ratio is not None and blank_ratio > 0.05):
                sev = "RED"
            elif (coverage is not None and coverage < 0.98) or (blank_ratio is not None and blank_ratio > 0.02):
                sev = "YELLOW"

            details.append({
                "from": f"{ftab}[{fcol}]",
                "to": f"{dtab}[{dcol}]",
                "blank_fk_ratio": round(blank_ratio, 4) if blank_ratio is not None else None,
                "coverage": round(coverage, 4) if coverage is not None else None,
                "severity": sev,
                "inactive": False,
                "hint": None
            })

        return {"summary": details}

    # ======== å¥‘çº¦ç»„è£… ========
    def _build_llm_contract(
        self,
        md: Dict[str, Any],
        st: Dict[str, Any],
        profiles: Dict[str, Any],
        *,
        include_measure_dax: bool,
        include_enums: bool,
        max_enum_values: int
    ) -> Dict[str, Any]:
        # ç»´åº¦å±•ç¤ºåˆ— + åˆ«å
        dimensions: Dict[str, Any] = {}
        for t, ty in st['table_types'].items():
            if ty != 'dimension': continue
            label = self._pick_label_column(t, md)
            alias_target = f"{t}[{label}]" if label else None
            aliases = self._expand_synonyms(label or t.replace('vwpcse_', ''))
            alias_map = {a: alias_target for a in aliases if alias_target}
            dimensions[t] = {"label": label, "aliases": alias_map}

        # äº‹å®æ‘˜è¦ï¼ˆæ—¶é—´é”®ã€æ—¥æœŸç»´åº¦ã€group-by å»ºè®®ï¼‰
        facts: Dict[str, Any] = {}
        for f, ty in st['table_types'].items():
            if ty != 'fact': continue
            ft = st['fact_time'].get(f, {}) if st.get('fact_time') else {}
            group_by = self._suggest_group_by(f, st, md)
            facts[f] = {
                "default_time_key": ft.get('default_time_key'),
                "date_dimension": ft.get('date_dimension') or st['date_axis']['table'],
                "date_dimension_key": ft.get('date_dimension_key') or st['date_axis']['key_column'],
                "group_by_suggestions": group_by[:5] if group_by else []
            }

        # å…³ç³»ï¼ˆå«éæ´»åŠ¨ USERELATIONSHIP æç¤ºï¼‰
        relationships = []
        for r in md['relationships']:
            if self._is_auto_table(r.get('from_table')) or self._is_auto_table(r.get('to_table')):
                continue
            inactive = not self._b(r.get('is_active'))
            hint = None
            if inactive:
                ft, fc, tt, tc = r.get('from_table'), r.get('from_column'), r.get('to_table'), r.get('to_column')
                if ft and fc and tt and tc:
                    hint = f"USERELATIONSHIP('{ft}'[{fc}], '{tt}'[{tc}])"
            relationships.append({
                "from": f"{r.get('from_table')}[{r.get('from_column')}]",
                "to": f"{r.get('to_table')}[{r.get('to_column')}]",
                "direction": r.get('cross_filter_direction'),
                "inactive": inactive,
                "hint": hint
            })

        # åº¦é‡ï¼ˆåªç»™ç±»åˆ«/ä¾èµ–ï¼›å¯å¼€å…³è¾“å‡º DAXï¼‰
        measures: Dict[str, Any] = {}
        for m in md['measures']:
            if self._b(m.get('is_hidden')):  # éšè—çš„ç•¥è¿‡
                continue
            name = m.get('measure_name')
            if not name: continue
            dax = m.get('dax_expression') or ''
            cat = self._measure_category(dax)
            dep = self._extract_measure_deps(dax)
            entry = {
                "table": m.get('table_name'),
                "category": cat,
                "format": m.get('format_string') or '',
                "depends_on": dep
            }
            if include_measure_dax:
                entry["dax"] = dax
            measures[name] = entry

        # æšä¸¾ï¼ˆå¯é€‰ï¼Œå¸¸è§ä¸šåŠ¡åˆ—ï¼‰
        enums: Dict[str, List[Any]] = {}
        if include_enums:
            enum_candidates = [
                ('vwpcse_factescalationcase', 'EscalationTier'),
                ('vwpcse_facttask_created', 'TaskType'),
                ('vwpcse_factincident_closed', 'Case State')
            ]
            for t, c in enum_candidates:
                dax = f"""
EVALUATE
TOPN({max_enum_values},
  SUMMARIZE('{t}', '{t}'[{c}], "cnt", COUNTROWS('{t}')),
  [cnt], DESC
)"""
                try:
                    df = self.runner.evaluate(dataset=md.get('model_name') or "", dax=dax, workspace=None)
                    if not df.empty:
                        vals = [row[0] for row in df.iloc[:, :1].values.tolist() if row and row[0] is not None]
                        if vals:
                            enums[f"{t}[{c}]"] = vals
                except Exception:
                    pass

        # åˆæˆ
        contract: Dict[str, Any] = {
            "version": "llm-contract/1.1",
            "date_axis": st.get('date_axis'),
            "facts": facts,
            "dimensions": dimensions,
            "relationships": relationships,
            "measures": measures,
            "data_profile": profiles or {},
            "counts": {
                "tables": len(md.get('business_tables', [])),
                "measures": len(measures),
                "relationships_active_non_auto": len([r for r in md['relationships'] if self._active_business_rel(r)])
            },
            "warnings": self._model_warnings(dimensions)
        }
        return contract

    # ======== å·¥å…·å‡½æ•° ========
    def _active_business_rel(self, r: Dict[str, Any]) -> bool:
        if not self._b(r.get('is_active')): return False
        return not (self._is_auto_table(r.get('from_table')) or self._is_auto_table(r.get('to_table')))

    @staticmethod
    def _is_auto_table(name: Optional[str]) -> bool:
        if not name: return False
        return bool(re.match(r'^(LocalDateTable_|DateTableTemplate_)', name, re.IGNORECASE))

    @staticmethod
    def _b(v: Any) -> bool:
        if v is None: return False
        if isinstance(v, str):
            return v.strip().lower() in {"true","1","yes","y","t"}
        try:
            return bool(v)
        except Exception:
            return False

    @staticmethod
    def _to_iso(value: Any) -> Optional[str]:
        """å°†å¤šç§æ—¥æœŸ/æ—¶é—´å¯¹è±¡æ ‡å‡†åŒ–ä¸º ISO8601 å­—ç¬¦ä¸²ã€‚

        Args:
            value: ä»»æ„å¾…åºåˆ—åŒ–å¯¹è±¡ï¼Œå¸¸è§ä¸º pandas.Timestampã€datetimeã€numpy.datetime64ã€‚

        Returns:
            å¯ç›´æ¥å†™å…¥ JSON çš„ ISO8601 å­—ç¬¦ä¸²ï¼›è‹¥æ— æ³•è¯†åˆ«åˆ™è¿”å› str(value)ã€‚
        """
        # None ç›´æ¥è¿”å›ï¼Œä¿æŒæ•°æ®ç¼ºå¤±è¯­ä¹‰ã€‚
        if value is None:
            return None

        # å…ˆå¤„ç† pandas.Timestampï¼Œå…¼å®¹ NaTã€‚
        if isinstance(value, pd.Timestamp):
            if pd.isna(value):
                return None
            return value.to_pydatetime().isoformat()

        # å¤„ç† Python åŸç”Ÿæ—¥æœŸ/æ—¶é—´ç±»å‹ã€‚
        if isinstance(value, (dt.datetime, dt.date)):
            return value.isoformat()

        # å¤„ç† NumPy datetime64ï¼ˆè‹¥å¯ç”¨ï¼‰ã€‚
        if _NUMPY_AVAILABLE and isinstance(value, np.datetime64):
            ts = pd.Timestamp(value)
            if pd.isna(ts):
                return None
            return ts.to_pydatetime().isoformat()

        # å…œåº•ï¼šè½¬æˆå­—ç¬¦ä¸²ï¼Œé¿å… json.dumps å¤±è´¥ã€‚
        return str(value)

    @staticmethod
    def _json_dumps(obj: Any) -> str:
        """åºåˆ—åŒ– JSONï¼ŒåŒæ—¶å®‰å…¨å¤„ç†æ—¶é—´å¯¹è±¡ä¸ NumPy æ ‡é‡ã€‚

        Args:
            obj: ä»»æ„å¯è¢« JSON åºåˆ—åŒ–çš„æ•°æ®ç»“æ„ã€‚

        Returns:
            ç»è¿‡ç¼©è¿›ä¸é ASCII å‹å¥½è®¾ç½®çš„ JSON å­—ç¬¦ä¸²ã€‚
        """

        def _default(o: Any) -> Any:
            """default å›è°ƒï¼šé™çº§å¤„ç†ç‰¹æ®Šå¯¹è±¡ã€‚"""
            # pandas.Timestampï¼Œå« NaTã€‚
            if isinstance(o, pd.Timestamp):
                if pd.isna(o):
                    return None
                return o.to_pydatetime().isoformat()

            # Python datetime/dateã€‚
            if isinstance(o, (dt.datetime, dt.date)):
                return o.isoformat()

            # NumPy æ ‡é‡ï¼ˆæ•°å­—/å¸ƒå°”/æ—¶é—´ï¼‰ã€‚
            if _NUMPY_AVAILABLE:
                if isinstance(o, np.integer):
                    return int(o)
                if isinstance(o, np.floating):
                    return float(o)
                if isinstance(o, np.bool_):
                    return bool(o)
                if isinstance(o, np.datetime64):
                    return LLMModelDocLite._to_iso(o)

            # å…¶ä»–å¯¹è±¡å¦‚æœæä¾› isoformat()ï¼Œå°è¯•è°ƒç”¨ã€‚
            if hasattr(o, "isoformat") and callable(o.isoformat):
                return o.isoformat()

            # æœ€åå…œåº•è½¬æˆå­—ç¬¦ä¸²ã€‚
            return str(o)

        return json.dumps(obj, ensure_ascii=False, indent=2, default=_default)

    @staticmethod
    def _to_int(x: Any) -> Optional[int]:
        if x is None: return None
        try:
            if isinstance(x, float) and pd.isna(x): return None
            if isinstance(x, str) and x.strip() == "": return None
            return int(x)
        except Exception:
            return None

    def _pick_label_column(self, table: str, md: Dict[str, Any]) -> Optional[str]:
        cols = [c for c in md['columns'] if c.get('table_name') == table and not self._b(c.get('is_hidden'))]
        text_cols = [c for c in cols if any(k in (c.get('data_type') or '').lower() for k in ['text','string'])]
        # ä¼˜å…ˆåŒ¹é… Name/Title
        for c in text_cols:
            n = (c.get('column_name') or '').lower()
            if n.endswith('name') or n.endswith('title'): return c.get('column_name')
        for kw in ['country','region','area','site','queue','category','partner','product','language']:
            for c in text_cols:
                if kw in (c.get('column_name') or '').lower(): return c.get('column_name')
        return text_cols[0]['column_name'] if text_cols else (cols[0]['column_name'] if cols else None)

    @staticmethod
    def _expand_synonyms(label: str) -> List[str]:
        if not label: return []
        base = label.replace('_',' ').strip()
        variants: Set[str] = {base, base.lower(), base.title()}
        mapping = {
            'queue': ['é˜Ÿåˆ—','Queue','ã‚­ãƒ¥ãƒ¼'],
            'country': ['å›½å®¶','Country','å›½'],
            'region': ['åŒºåŸŸ','Region','ãƒªãƒ¼ã‚¸ãƒ§ãƒ³'],
            'area': ['åœ°åŒº','Area','ã‚¨ãƒªã‚¢'],
            'site': ['ç«™ç‚¹','Site','ã‚µã‚¤ãƒˆ'],
            'partner': ['åˆä½œä¼™ä¼´','Partner','ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼'],
            'category': ['ç±»åˆ«','Category','ã‚«ãƒ†ã‚´ãƒª'],
            'product': ['äº§å“','Product','ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ'],
            'language': ['è¯­è¨€','Language','è¨€èª']
        }
        low = base.lower()
        for k, words in mapping.items():
            if k in low: variants.update(words)
        return sorted(variants)

    def _suggest_group_by(self, fact: str, st: Dict[str, Any], md: Dict[str, Any]) -> List[str]:
        dims = st['star'].get(fact, {}).get('dimensions', [])
        res: List[str] = []
        for d in dims:
            t = d.get('dimension_table')
            if not t: continue
            label = self._pick_label_column(t, md)
            if label:
                res.append(f"{t}[{label}]")
        return res

    @staticmethod
    def _measure_category(dax: str) -> str:
        d = (dax or '').lower()
        if re.search(r'\bsumx?\(', d): return 'aggregation'
        if re.search(r'\b(distinctcount|count)\b', d): return 'counting'
        if re.search(r'\b(average|median|medianx|stdevx?|variance|percentile)\b', d): return 'statistical'
        if 'calculate(' in d: return 'filtered'
        if re.search(r'\b(dateadd|sameperiod|datesytd|datesmtd|datesqtd)\b', d): return 'time_intelligence'
        if '/' in d or 'divide(' in d: return 'calculation'
        return 'other'

    @staticmethod
    def _extract_measure_deps(dax: str) -> Dict[str, List[str]]:
        if not dax: return {"measures": [], "columns": []}
        col_pairs = re.findall(r"'([^']+)'\[([^\]]+)\]", dax)
        col_refs = {f"{t}[{c}]" for t, c in col_pairs}
        col_names = {c for _, c in col_pairs}
        measure_candidates = re.findall(r'\[([^\[\]]+)\]', dax)
        meas = sorted({m for m in measure_candidates if m not in col_names})
        return {"measures": meas, "columns": sorted(col_refs)}

    def _model_warnings(self, dimensions: Dict[str, Any]) -> List[str]:
        warns: List[str] = []
        # Queue çš„åŒé”®æç¤ºï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'vwpcse_dimqueue' in dimensions:
            warns.append("DimQueue å¯èƒ½å­˜åœ¨ QueueKey/QueueID åŒé”®ï¼Œè¯·ä¼˜å…ˆç»Ÿä¸€ä¸º QueueKey æˆ–ä½¿ç”¨æ¡¥è¡¨ã€‚")
        return warns


# ----------------------------
# CLI / Demo
# ----------------------------
if __name__ == "__main__":
    # === ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹ä¸å·¥ä½œåŒº ===
    MODEL_NAME = "PCSE AI"       # è¯­ä¹‰æ¨¡å‹åç§°
    WORKSPACE_GUID = None        # å¦‚éœ€æŒ‡å®šå·¥ä½œåŒº GUIDï¼›None ä½¿ç”¨å½“å‰ä¸Šä¸‹æ–‡
    # ç”Ÿæˆè®¾ç½®
    INCLUDE_MEASURE_DAX = False  # True ä¼šæŠŠæ‰€æœ‰å¯è§åº¦é‡ DAX æ”¾è¿›å¥‘çº¦ï¼ˆå¯èƒ½å¾ˆé•¿ï¼‰
    PROFILE_MODE = "light"       # "off" | "light" | "standard"
    REL_TOP_K = 12               # standard æ¨¡å¼ä¸‹ä½“æ£€çš„è¾¹æ•°é‡ä¸Šé™
    INCLUDE_ENUMS = False        # å¦‚éœ€æšä¸¾ Top å€¼ï¼Œè®¾ True
    MAX_ENUM_VALUES = 10
    OUTPUT_PATH = "llm_contract.json"
    # ===========================

    doc = LLMModelDocLite(verbose=True)
    contract_json = doc.generate(
        model_name=MODEL_NAME,
        workspace=WORKSPACE_GUID,
        include_measure_dax=INCLUDE_MEASURE_DAX,
        profile_mode=PROFILE_MODE,
        relationship_top_k=REL_TOP_K,
        include_enums=INCLUDE_ENUMS,
        max_enum_values=MAX_ENUM_VALUES
    )
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(contract_json)
    print(f"\nâœ… LLM å¥‘çº¦å·²å†™å…¥ {OUTPUT_PATH}")
